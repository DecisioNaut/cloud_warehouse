{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration for Sparkify Data Model  \n",
    "In order to come up with a proper model, I downloaded the (currently relatively small) dataset, stored is as csv-files and did some exploration.  \n",
    "\n",
    "The targeted star schema is as follows:\n",
    "\n",
    "**Fact Table: `songplays`**\n",
    "- `session_id`\n",
    "- `songplay_id`\n",
    "- `start_time`\n",
    "- `artist_id`\n",
    "- `song_id`\n",
    "- `user_id`\n",
    "- `level`\n",
    "- `location`\n",
    "- `user_agent`\n",
    "\n",
    "**Dimension Table: `time`**\n",
    "- `start_time`\n",
    "- `year`\n",
    "- `month`\n",
    "- `day`\n",
    "- `hour`\n",
    "- `week`\n",
    "- `weekday`\n",
    "\n",
    "**Dimension Table: `artists`**\n",
    "- `artist_id`\n",
    "- `name`\n",
    "- `location`\n",
    "- `lattitude`\n",
    "- `longitude`\n",
    "\n",
    "**Dimension Table: `songs`**\n",
    "- `song_id`\n",
    "- `title`\n",
    "- `artist_id`\n",
    "- `year`\n",
    "- `duration`\n",
    "\n",
    "**Dimension Table: `users`**\n",
    "- `user_id`\n",
    "- `first_name`\n",
    "- `last_name`\n",
    "- `gender`\n",
    "- `level`\n",
    "\n",
    "The source data is stored in S3 buckets. The log data is stored in `s3://udacity-dend/log_data` and the song data is stored in `s3://udacity-dend/song_data`. Both is stored in json format. The log data contains information about the user activity on the Sparkify app. The song data contains additional information about the songs that are available in the Sparkify app.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Observations and Findings  \n",
    "\n",
    "### Structure of the `log_data` files\n",
    "\n",
    "The `log_data` files contain 8056 entries with the following fields:  \n",
    "\n",
    "- Fields for later direct use:\n",
    "    - Identifiers / keys:\n",
    "        - `sessionId`: Session ID as an integer\n",
    "        - `itemInSession`: Item in session as an integer\n",
    "    - Timestamp:\n",
    "        - `ts`: Timestamp as a long integer being the number of milliseconds since 1.1.1970\n",
    "    - Artist:\n",
    "        - `artist`: Name of the Artist as a string\n",
    "    - Song:\n",
    "        - `song`: Song title as a string\n",
    "    - User:\n",
    "        - `userId`: User ID as a string\n",
    "        - `firstName`: First name of the user as a string\n",
    "        - `lastName`: Last name of the user as a string\n",
    "        - `gender`: Gender of the user as a string being either \"M\" or \"F\"\n",
    "        - `level`: Level of the user as a string being either \"free\" or \"paid\"\n",
    "    - Other usage data:\n",
    "        - `location`: Location of the user as a string\n",
    "        - `userAgent`: User agent (browser) as a string\n",
    "- Fields for later pre-processing:  \n",
    "    - `auth`: Authentication status as a string being either \"Logged In\" or \"Logged Out\"  \n",
    "    - `length`: Length of the playing of the songs as a float  \n",
    "- Other fields not used later:  \n",
    "    - `method`: Method as a string being either \"GET\" or \"PUT\"  \n",
    "    - `page`: Page as a string  \n",
    "    - `registration`: Registration as a float  \n",
    "    - `status`: Status message as an integer  \n",
    "\n",
    "### Key finding regarding to the `log_data` files:\n",
    "- `sessionId` and `itemInSession` in combination can be used as primary key for the fact table.\n",
    "- Relevant facts like song and artist information are only given when \n",
    "    - the user is not logged off, \n",
    "    - the lenght of the playing is not zero  \n",
    "    This means, we should pre-filter the data accordingly.\n",
    "- With there filters all other data is available / not missing.\n",
    "- The combination of `userId`, `firstName`, `lastName`, `gender` and `level` is not unique as users change their `level` over time. As a consequence, I've decided to use `userId` as primary key for the users dimension table taking the latest available `level` into account. However, the `level` in the fact table reflects the `level` at the time of the song play.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure of the `song_data` files\n",
    "\n",
    "The `song_data` files contain 14896 entries with the following fields:\n",
    "\n",
    "- Song related fields:\n",
    "    - `song_id`: Song ID as a string with 18 characters\n",
    "    - `title`: Song title as a string\n",
    "    - `year`: Year of the song as an integer with, in general, four digits, but some being 0, meaning the year is unknown / missing\n",
    "    - `duration`: Duration of the song as a float\n",
    "- Artist related fields:\n",
    "    - `artist_id`: Artist ID as a string with 18 character\n",
    "    - `artist_name`: Name of the artist as a string\n",
    "    - `artist_location`: Location of the artist as a string with many missing values\n",
    "    - `artist_latitude`: Latitude of the artist as a float with many missing values\n",
    "    - `artist_longitude`: Longitude of the artist as a float with many missing values\n",
    "\n",
    "### Key finding regarding the `song data` files:\n",
    "\n",
    "- One may be tempted to use this data to build the dimension tables `songs` and `artists`, however, this would be a bad idea as data quality in combination with the `log_data` files is not good. There are many songs and artists in the `log_data` files that are not available in the `song_data` files. \n",
    "- This means, we use the detail available in the `song_data` files only to enrich the data in the `log_data` files where possible.\n",
    "- In addition to this, `artist_id`, `artist_name`, `artist_location`, `artist_latitude` and `artist_longitude` are not a unique set. There are artists having various location and inconsistent geographical coordinates. When using this data to enrich the `log_data` files, we should be aware of this. Here, I've decided to use the first available entry for each artist."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Conclusion for the Data Model\n",
    "\n",
    "- Due to consistency issues, our main source is the `log_data` files.\n",
    "- The `song_data` files are only used to enrich the data in the `log_data` files where possible.\n",
    "- The `log_data` files are pre-filtered to only contain entries where the user is logged in and the length of the playing is not zero."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration in Detail"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import csv\n",
    "import json\n",
    "import sqlite3\n",
    "from typing import Tuple\n",
    "\n",
    "import boto3\n",
    "from dotenv import dotenv_values\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set max rows to 30\n",
    "pd.set_option('display.max_rows', 30)\n",
    "\n",
    "# Get credentials from .env file\n",
    "env = dotenv_values()\n",
    "\n",
    "AWS_ACCESS_KEY_ID = env[\"AWS_ACCESS_KEY_ID\"]\n",
    "AWS_SECRET_ACCESS_KEY = env[\"AWS_SECRET_ACCESS_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to S3\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=\"us-west-2\",\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split bucket string into name and prefix\n",
    "def bucket_name_and_prefix_from_string(bucket_string: str) -> Tuple[str, str]:\n",
    "    \"\"\"Creates a filter for the bucket.\"\"\"\n",
    "    bucket_name = bucket_string.split(\"/\")[2]\n",
    "    prefix = f\"{'/'.join(bucket_string.split('/')[3:])}/\"\n",
    "    return bucket_name, prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load log data into a list\n",
    "log_data = []\n",
    "\n",
    "bucket_name, prefix = bucket_name_and_prefix_from_string('s3://udacity-dend/log_data')\n",
    "bucket_name, prefix\n",
    "\n",
    "paginator = s3.get_paginator(\"list_objects\")\n",
    "page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "for page in page_iterator:\n",
    "    for item in page[\"Contents\"]:\n",
    "        key = item[\"Key\"]\n",
    "        if key != prefix:\n",
    "            object_body = s3.get_object(Bucket=bucket_name, Key=key)[\"Body\"].read().decode(\"utf-8\")\n",
    "            for data in object_body.split(\"\\n\"):\n",
    "                if data:\n",
    "                    log_data.append(json.loads(data))\n",
    "\n",
    "# Show length of log data\n",
    "len(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show an example of the log data\n",
    "log_data[4_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put log data into a dataframe and save to csv\n",
    "all_log_data = pd.DataFrame(log_data)\n",
    "all_log_data.to_csv(\"./data/project/log_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load song data into a list\n",
    "song_data = []\n",
    "\n",
    "bucket_name, prefix = bucket_name_and_prefix_from_string('s3://udacity-dend/song_data')\n",
    "bucket_name, prefix\n",
    "\n",
    "paginator = s3.get_paginator(\"list_objects\")\n",
    "page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "for page in page_iterator:\n",
    "    for item in page[\"Contents\"]:\n",
    "        key = item[\"Key\"]\n",
    "        if key != prefix:\n",
    "            object_body = s3.get_object(Bucket=bucket_name, Key=key)[\"Body\"].read().decode(\"utf-8\")\n",
    "            for data in object_body.split(\"\\n\"):\n",
    "                if data:\n",
    "                    song_data.append(json.loads(data))\n",
    "\n",
    "# Show length of song data\n",
    "len(song_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show an example of the song data\n",
    "song_data[4_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put song data into a dataframe and save to csv\n",
    "all_song_data = pd.DataFrame(song_data)\n",
    "all_song_data.to_csv(\"./data/project/song_data.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the center of the data model is the fact table `songplays` containing information about the user activity. So let's start here:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration of `log_data` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_log_data = pd.read_csv('./data/project/log_data.csv')\n",
    "all_log_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_log_data.describe(include=\"all\").T.fillna(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** - Are `sessionId` and `itemInSession` applicable primary keys for this table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_log_data[[\"sessionId\", \"itemInSession\"]].shape[0] == all_log_data.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** - What's the min and max lenght of the strings in the table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in all_log_data.select_dtypes(include=['object']).columns:\n",
    "    print(column, all_log_data[column].dropna().map(lambda x: len(str(x))).min(), all_log_data[column].dropna().map(lambda x: len(str(x))).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_columns_required = [\n",
    "    # Data for primary keys\n",
    "    \"sessionId\", \n",
    "    \"itemInSession\", \n",
    "    # Data for timestamp\n",
    "    \"ts\",\n",
    "    # User related data\n",
    "    \"userId\",\n",
    "    \"firstName\", \n",
    "    \"lastName\",\n",
    "    \"gender\",\n",
    "    \"level\", # also usage related\n",
    "    # Usage related data\n",
    "    \"location\",\n",
    "    \"userAgent\",\n",
    "    # Song related data\n",
    "    \"song\", \n",
    "    # Artist related data\n",
    "    \"artist\",\n",
    "]\n",
    "\n",
    "log_data = all_log_data[log_columns_required]\n",
    "log_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data.describe(include='all').T.fillna(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** - Can we use `sessionId` and `itemInSession` as primary key for the fact table?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data[['sessionId', 'itemInSession']].drop_duplicates().shape[0] == log_data.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** - What's the structure of missing data?**\n",
    "\n",
    "- Subquestion: Is `userId`, `firstName`, `lastName`, `gender`, `location` and `userAgend` missing always for the same rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data[[\"userId\", \"firstName\", \"lastName\", \"gender\", \"location\", \"userAgent\"]].dropna().shape[0] == log_data[\"userId\"].dropna().shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Subquestion: Is there something special about the missing `userId` data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_log_data[all_log_data[\"userId\"].isna()].drop([\"userId\", \"firstName\", \"lastName\", \"gender\", \"location\", \"userAgent\"], axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_log_data[all_log_data[\"userId\"].isna()].drop([\"userId\", \"firstName\", \"lastName\", \"gender\", \"location\", \"userAgent\"], axis=1).describe(include='all').T.fillna(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Subquestion: Are events with not missing `userId` having `auth` == \"Logged In\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_log_data.loc[log_data[\"userId\"].notna(), \"auth\"].unique().tolist() == [\"Logged In\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FINDING** - log_data with `auth` == \"Logged Out\" can be dropped as it doesn't contain any useful information for the data model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** - Is `song` and `artist` always missing for the same rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data[[\"song\", \"artist\"]].dropna().shape[0] == log_data[\"song\"].dropna().shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** - Are the not missing `song` occuring when `userId` is missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data.loc[log_data[\"song\"].notna() & log_data[\"userId\"].isna()].shape[0] > 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** - Is there something special about the missing `song` and `artist` data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_log_data.loc[all_log_data[\"song\"].isna()].drop([\"song\", \"artist\"], axis=1).describe(include='all').T.fillna(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** - Is the `length`== 0 when `song` is not missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(all_log_data.loc[all_log_data[\"song\"].notna()].drop([\"song\", \"artist\"], axis=1)[\"length\"] == 0).any() == False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FINDING** - log_data with `length` == 0 can be dropped as it doesn't contain any useful information for the data model.\n",
    "\n",
    "So, let's filter the data accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_log_data = all_log_data.loc[(all_log_data[\"auth\"] != \"Logged Out\") & (all_log_data[\"length\"] > 0), log_columns_required]\n",
    "filtered_log_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_log_data.describe(include='all').T.fillna(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** - What are the times from `ts` the songs are heard? Is this consistent with the log_data file organisation covering the month of November 2018?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(filtered_log_data[\"ts\"], unit='ms').describe(datetime_is_numeric=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** - What are the locations the songs are heard?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_log_data[\"location\"].value_counts()[:25]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** - What are the user agents the songs are heard with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_log_data[\"userAgent\"].value_counts()[:25]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Closer look at the user related data in `log_data`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have a closer look at the user related data contained in the log data, namely: \n",
    "- `userId`, \n",
    "- `firstName`, \n",
    "- `lastName`, \n",
    "- `gender`, and \n",
    "- `level`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = filtered_log_data[[\"userId\", \"firstName\", \"lastName\", \"gender\", \"level\"]].drop_duplicates().sort_values(\"userId\")\n",
    "user_data[\"userId\"] = user_data[\"userId\"].astype(int).astype(str)\n",
    "user_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data.describe(include='all').T.fillna(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** - Where do duplicates in user_data come from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data[user_data.duplicated(subset=[\"userId\"], keep=False)].sort_values([\"userId\", \"level\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FINDING** - The duplicates in user_data come from the fact that the user can change his/her subscription level."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Closer look at the song and artist related data in `log_data`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** - What's the structure of song related data in the log_data, namely `song` and `artist`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_from_filtered_log_data = filtered_log_data.loc[:, [\"song\", \"artist\"]].sort_values([\"song\", \"artist\"]).drop_duplicates()\n",
    "song_from_filtered_log_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_from_filtered_log_data.describe(include='all').T.fillna(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** - What the structure of the duplicate song in the song_data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_from_filtered_log_data.loc[song_from_filtered_log_data[\"song\"].duplicated(keep=False)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FINDING** - To identify the song in the log data, we need both `song` and `artist` from the log_data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** - Is the filtering giving back valid data for the required fields?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(all_log_data.query(\"auth == 'Logged In' and length > 0\")[[\"ts\", \"userId\", \"artist\", \"song\"]].count() == 6820).all()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration of `song_data` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_song_data = pd.read_csv('./data/project/song_data.csv')\n",
    "all_song_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_song_data.describe(include='all').T.fillna(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** - Are artist_id and song_id applicable primary keys for this table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_song_data[[\"song_id\", \"artist_id\"]].shape[0] == all_song_data.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** - What the lenght of the strings in the song_data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in all_song_data.select_dtypes(\"object\"):\n",
    "    print(column, all_song_data[column].dropna().map(lambda x: len(str(x))).min(), all_song_data[column].dropna().map(lambda x: len(str(x))).max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form the requirements of the project, we know that we might need the following columns:\n",
    "- `song_id`\n",
    "- `title`\n",
    "- `year`\n",
    "- `duration`\n",
    "- `artist_id`\n",
    "- `artist_name`\n",
    "- `artist_location`\n",
    "- `artist_latitude`\n",
    "\n",
    "This means, we can drop `num_songs` from the table.\n",
    "\n",
    "So let's narrow the dataset a bit down:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_required = ['song_id', 'title', 'year', 'duration', 'artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude']\n",
    "song_data = all_song_data[columns_required]\n",
    "song_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data.describe(include='all').T.fillna(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** - Is song_id unique for title and artist_name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data[[\"title\", \"artist_name\"]].drop_duplicates().shape[0] == song_data.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** - What's the reason for the duplicates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data.loc[song_data.duplicated(subset=[\"title\", \"artist_name\"], keep=False)].sort_values([\"title\", \"artist_name\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FINDING** - title and artist_name are not unique in relation to song_id as there are ambiguous entries for the duration."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Closer look at the artist related data in `song_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists_from_song_data = song_data[[\"artist_id\", \"artist_name\", \"artist_location\", \"artist_latitude\", \"artist_longitude\"]].drop_duplicates()\n",
    "artists_from_song_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** - Are the artist_id unique for artist_name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists_from_song_data[[\"artist_id\", \"artist_name\"]].drop_duplicates().shape[0] == artists_from_song_data[\"artist_id\"].drop_duplicates().shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** - What the reason of non-unique artist_ids?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists_from_song_data.loc[artists_from_song_data.duplicated(subset=[\"artist_name\"], keep=False)].sort_values([\"artist_name\"]).iloc[:30]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FINDING** - Non-unique artist_ids are due to the fact that there are artists with different locations and/or geographical coordinates, probably due to ambiguity in the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Information between `log_data` and `song_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_song_from_log = all_log_data.query(\"(auth != 'Logged Out') & (length > 0)\")[[\"artist\", \"song\"]].drop_duplicates().sort_values([\"artist\", \"song\"]).reset_index(drop=True).reset_index().rename(columns={\"index\": \"from_log\"})\n",
    "artist_song_from_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_song_from_songs = all_song_data[[\"artist_name\", \"title\"]].drop_duplicates().sort_values([\"artist_name\", \"title\"]).reset_index(drop=True).reset_index().rename(columns={\"artist_name\": \"artist\", \"title\": \"song\", \"index\": \"from_songs\"})\n",
    "artist_song_from_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_song_from_log.merge(artist_song_from_songs, on=[\"artist\", \"song\"], how=\"right\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** - How many songs are in the log_data that are also in the song_data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_song_from_log.merge(artist_song_from_songs, on=[\"artist\", \"song\"], how=\"right\")[\"from_log\"].count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FINDING** - The overlap between the log_data and the song_data is pretty low."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating users_df\n",
    "users_df = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"user_id\",\n",
    "        \"first_name\",\n",
    "        \"last_name\",\n",
    "        \"gender\",\n",
    "        \"level\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Filling users_df\n",
    "users_df = (\n",
    "    all_log_data\n",
    "    .query(\"(auth != 'Logged Out') & (length > 0)\")\n",
    "    [[\"userId\", \"firstName\", \"lastName\", \"gender\", \"level\", \"ts\"]]\n",
    "    .rename(columns={\"userId\": \"user_id\", \"firstName\": \"first_name\", \"lastName\": \"last_name\"})\n",
    "    .sort_values(\"ts\")\n",
    "    .drop_duplicates(subset=[\"user_id\", \"first_name\", \"last_name\", \"gender\"], keep=\"last\")\n",
    "    .drop(\"ts\", axis=1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "users_df[\"user_id\"] = users_df[\"user_id\"].astype(int)\n",
    "\n",
    "# Showing users_df\n",
    "users_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating time_df\n",
    "time_df = pd.DataFrame(\n",
    "    columns=[\n",
    "        \n",
    "        \"start_time\",\n",
    "        \"year\",\n",
    "        \"month\",\n",
    "        \"day\",\n",
    "        \"hour\",\n",
    "        \"week\",\n",
    "        \"weekday\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Filling time_df\n",
    "time_df[\"start_time\"] = pd.to_datetime(\n",
    "    all_log_data.query(\"(auth != 'Logged Out') & (length > 0)\")[\"ts\"].drop_duplicates().sort_values(),\n",
    "    unit=\"ms\",\n",
    ")\n",
    "\n",
    "time_df[\"year\"] = pd.to_datetime(\n",
    "    all_log_data.query(\"(auth != 'Logged Out') & (length > 0)\")[\"ts\"].drop_duplicates().sort_values(),\n",
    "    unit=\"ms\",\n",
    ").dt.isocalendar().year\n",
    "\n",
    "time_df[\"month\"] = pd.to_datetime(\n",
    "    all_log_data.query(\"(auth != 'Logged Out') & (length > 0)\")[\"ts\"].drop_duplicates().sort_values(),\n",
    "    unit=\"ms\",\n",
    ").dt.month\n",
    "\n",
    "time_df[\"day\"] = pd.to_datetime(\n",
    "    all_log_data.query(\"(auth != 'Logged Out') & (length > 0)\")[\"ts\"].drop_duplicates().sort_values(),\n",
    "    unit=\"ms\",\n",
    ").dt.day\n",
    "\n",
    "time_df[\"hour\"] = pd.to_datetime(\n",
    "    all_log_data.query(\"(auth != 'Logged Out') & (length > 0)\")[\"ts\"].drop_duplicates().sort_values(),\n",
    "    unit=\"ms\",\n",
    ").dt.hour\n",
    "\n",
    "time_df[\"week\"] = pd.to_datetime(\n",
    "    all_log_data.query(\"(auth != 'Logged Out') & (length > 0)\")[\"ts\"].drop_duplicates(),\n",
    "    unit=\"ms\",\n",
    ").dt.isocalendar().week\n",
    "\n",
    "time_df[\"weekday\"] = pd.to_datetime(\n",
    "    all_log_data.query(\"(auth != 'Logged Out') & (length > 0)\")[\"ts\"].drop_duplicates(),\n",
    "    unit=\"ms\",\n",
    ").dt.weekday\n",
    "\n",
    "# Showing time_df\n",
    "time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating artists_df\n",
    "artists_df = pd.DataFrame(\n",
    "    columns=[\n",
    "        # \"artist_id\",\n",
    "        \"name\",\n",
    "        #\"location\",\n",
    "        #\"latitude\",\n",
    "        #\"longitude\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Filling artists_df\n",
    "artists_df[\"name\"] = all_log_data.query(\"(auth != 'Logged Out') & (length > 0)\")[\"artist\"].drop_duplicates().sort_values()\n",
    "\n",
    "additional_artist_info = (\n",
    "    all_song_data\n",
    "    [[\"artist_name\", \"artist_location\", \"artist_latitude\", \"artist_longitude\"]]\n",
    "    .rename(columns={\"artist_name\": \"name\", \"artist_location\": \"location\", \"artist_latitude\": \"latitude\", \"artist_longitude\": \"longitude\"})\n",
    "    .sort_values([\"name\", \"location\", \"latitude\", \"longitude\"])\n",
    "    .drop_duplicates(subset=[\"name\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "artists_df = artists_df.merge(additional_artist_info, on=\"name\", how=\"left\")\n",
    "artists_df = artists_df.reset_index().rename(columns={\"index\": \"artist_id\"})\n",
    "\n",
    "# Showing artists_df\n",
    "artists_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating songs_df\n",
    "songs_df = all_log_data.query(\"(auth != 'Logged Out') & (length > 0)\")[[\"song\", \"artist\"]].drop_duplicates().sort_values([\"song\"]).rename(columns={\"song\": \"title\", \"artist\": \"name\"}).reset_index(drop=True).reset_index().rename(columns={\"index\": \"song_id\"})\n",
    "\n",
    "songs_df = songs_df.merge(artists_df[[\"name\", \"artist_id\"]], on=\"name\", how=\"left\")\n",
    "songs_df\n",
    "\n",
    "songs_df = songs_df.merge(\n",
    "    all_song_data[[\"artist_name\", \"title\", \"year\", \"duration\"]].rename(columns={\"artist_name\": \"name\"}), \n",
    "    on=[\"name\", \"title\"], \n",
    "    how=\"left\"\n",
    ").drop(\"name\", axis=1)\n",
    "\n",
    "# Showing songs_df\n",
    "songs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_1_df = songs_df.merge(artists_df, on=\"artist_id\", how=\"left\")[[\"song_id\", \"title\", \"artist_id\", \"name\"]].rename(columns={\"name\": \"artist\", \"title\": \"song\"})\n",
    "helper_2_df = all_log_data.query(\"(auth != 'Logged Out') & (length > 0)\")[[\"artist\", \"song\"]].merge(helper_1_df, on=[\"artist\", \"song\"], how=\"left\").drop([\"artist\", \"song\"], axis=1)\n",
    "helper_2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating songplays_df\n",
    "songplays_df = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"session_id\",\n",
    "        \"songplay_id\",\n",
    "        \"start_time\",\n",
    "        \"song_id\",\n",
    "        \"artist_id\",\n",
    "        \"user_id\",\n",
    "        \"location\",\n",
    "        \"level\",\n",
    "        \"user_agent\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Filling songplays_df\n",
    "songplays_df[\"session_id\"] = all_log_data.query(\"(auth != 'Logged Out') & (length > 0)\")[\"sessionId\"]\n",
    "songplays_df[\"songplay_id\"] = all_log_data.query(\"(auth != 'Logged Out') & (length > 0)\")[\"itemInSession\"]\n",
    "songplays_df[\"start_time\"] = pd.to_datetime(all_log_data.query(\"(auth != 'Logged Out') & (length > 0)\")[\"ts\"], unit=\"ms\")\n",
    "\n",
    "helper_1_df = songs_df.merge(artists_df, on=\"artist_id\", how=\"left\")[[\"song_id\", \"title\", \"artist_id\", \"name\"]].rename(columns={\"name\": \"artist\", \"title\": \"song\"})\n",
    "helper_2_df = all_log_data.query(\"(auth != 'Logged Out') & (length > 0)\")[[\"artist\", \"song\"]].merge(helper_1_df, on=[\"artist\", \"song\"], how=\"left\").drop([\"artist\", \"song\"], axis=1)\n",
    "\n",
    "songplays_df[\"song_id\"] = helper_2_df[\"song_id\"].values\n",
    "songplays_df[\"artist_id\"] = helper_2_df[\"artist_id\"].values\n",
    "\n",
    "songplays_df[\"user_id\"] = all_log_data.query(\"(auth != 'Logged Out') & (length > 0)\")[\"userId\"].astype(int)\n",
    "songplays_df[\"location\"] = all_log_data.query(\"(auth != 'Logged Out') & (length > 0)\")[\"location\"]\n",
    "songplays_df[\"level\"] = all_log_data.query(\"(auth != 'Logged Out') & (length > 0)\")[\"level\"]\n",
    "songplays_df[\"user_agent\"] = all_log_data.query(\"(auth != 'Logged Out') & (length > 0)\")[\"userAgent\"]\n",
    "\n",
    "songplays_df = songplays_df.reset_index(drop=True)\n",
    "\n",
    "# Showing songplays_df\n",
    "songplays_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Using SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect(\"./data/project/sparkify.sqlite3\")\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete staging tables\n",
    "drop_log_data_table = \"DROP TABLE IF EXISTS log_data;\"\n",
    "drop_song_data_table = \"DROP TABLE IF EXISTS song_data;\"\n",
    "\n",
    "# Delete dimension tables\n",
    "drop_time_table = \"DROP TABLE IF EXISTS time;\"\n",
    "drop_users_table = \"DROP TABLE IF EXISTS users;\"\n",
    "drop_songs_table = \"DROP TABLE IF EXISTS songs;\"\n",
    "drop_artists_table = \"DROP TABLE IF EXISTS artists;\"\n",
    "\n",
    "# Delete fact table\n",
    "drop_songplays_table = \"DROP TABLE IF EXISTS songplays;\"\n",
    "\n",
    "# Drop all tables\n",
    "drop_tables = [\n",
    "    drop_log_data_table,\n",
    "    drop_song_data_table,\n",
    "    drop_time_table,\n",
    "    drop_users_table,\n",
    "    drop_songs_table,\n",
    "    drop_artists_table,\n",
    "    drop_songplays_table,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create staging tables\n",
    "create_log_data_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS log_data (\n",
    "    artist          VARCHAR(200)    NULL,\n",
    "    auth            VARCHAR(50)     NOT NULL,\n",
    "    firstName       VARCHAR(50)     NULL,\n",
    "    gender          CHAR(1)         NULL,\n",
    "    itemInSession   INTEGER         NOT NULL,\n",
    "    lastName        VARCHAR(50)     NULL,\n",
    "    length          FLOAT           NULL,\n",
    "    level           CHAR(4)         NOT NULL,\n",
    "    location        VARCHAR(200)    NULL,\n",
    "    method          VARCHAR(10)     NOT NULL,\n",
    "    page            VARCHAR(50)     NOT NULL,\n",
    "    registration    FLOAT           NULL,\n",
    "    sessionId       INTEGER         NOT NULL,\n",
    "    song            VARCHAR(200)    NULL,\n",
    "    status          INTEGER         NOT NULL,\n",
    "    ts              INTEGER         NOT NULL,\n",
    "    userAgent       VARCHAR(200)    NULL,\n",
    "    userId          INTEGER         NULL,\n",
    "    PRIMARY KEY (sessionId, itemInSession)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "create_song_data_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS song_data (\n",
    "    artist_id       VARCHAR(50)     NOT NULL,\n",
    "    artist_latitude FLOAT           NULL,\n",
    "    artist_location VARCHAR(200)    NULL,\n",
    "    artist_longitude FLOAT          NULL,\n",
    "    artist_name     VARCHAR(200)    NOT NULL,\n",
    "    duration        FLOAT           NOT NULL,\n",
    "    num_songs       INTEGER         NOT NULL,\n",
    "    song_id         VARCHAR(50)     NOT NULL,\n",
    "    title           VARCHAR(200)    NOT NULL,\n",
    "    year            INTEGER         NOT NULL,\n",
    "    PRIMARY KEY (artist_id, song_id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Create dimension tables\n",
    "create_time_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS time (\n",
    "    start_time      TIMESTAMP       NOT NULL,\n",
    "    year            INTEGER         NOT NULL,\n",
    "    month           INTEGER         NOT NULL,\n",
    "    day             INTEGER         NOT NULL,\n",
    "    hour            INTEGER         NOT NULL,\n",
    "    week            INTEGER         NOT NULL,\n",
    "    weekday         INTEGER         NOT NULL,\n",
    "    PRIMARY KEY (start_time)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "create_users_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS users (\n",
    "    user_id         INTEGER         NOT NULL,\n",
    "    first_name      VARCHAR(50)     NOT NULL,\n",
    "    last_name       VARCHAR(50)     NOT NULL,\n",
    "    gender          CHAR(1)         NOT NULL,\n",
    "    level           CHAR(4)         NOT NULL,\n",
    "    PRIMARY KEY (user_id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "create_artists_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS artists (\n",
    "    artist_id       INTEGER         NOT NULL,\n",
    "    name            VARCHAR(200)    NOT NULL,\n",
    "    location        VARCHAR(200)    NULL,\n",
    "    latitude        FLOAT           NULL,\n",
    "    longitude       FLOAT           NULL,\n",
    "    PRIMARY KEY (artist_id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "create_songs_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS songs (\n",
    "    song_id         INTEGER         NOT NULL,\n",
    "    title           VARCHAR(200)    NOT NULL,\n",
    "    artist_id       INTEGER         NOT NULL,\n",
    "    year            INTEGER         NULL,\n",
    "    duration        FLOAT           NULL,\n",
    "    PRIMARY KEY (song_id),\n",
    "    FOREIGN KEY (artist_id) REFERENCES artists (artist_id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Create fact table\n",
    "create_songplays_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS songplays (\n",
    "    session_id      INTEGER         NOT NULL,\n",
    "    songplay_id     INTEGER         NOT NULL,\n",
    "    start_time      TIMESTAMP       NOT NULL,\n",
    "    artist_id       INTEGER         NOT NULL,\n",
    "    song_id         INTEGER         NOT NULL,\n",
    "    user_id         INTEGER         NOT NULL,\n",
    "    level           CHAR(4)         NOT NULL,\n",
    "    location        VARCHAR(200)    NOT NULL,\n",
    "    user_agent      VARCHAR(200)    NOT NULL,\n",
    "    PRIMARY KEY (session_id, songplay_id),\n",
    "    UNIQUE (session_id, songplay_id),\n",
    "    FOREIGN KEY (start_time) REFERENCES time (start_time),\n",
    "    FOREIGN KEY (artist_id) REFERENCES artists (artist_id),\n",
    "    FOREIGN KEY (song_id) REFERENCES songs (song_id),\n",
    "    FOREIGN KEY (user_id) REFERENCES users (user_id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Create all tables\n",
    "create_tables = [\n",
    "    create_log_data_table,\n",
    "    create_song_data_table,\n",
    "    create_time_table,\n",
    "    create_users_table,\n",
    "    create_artists_table,\n",
    "    create_songs_table,\n",
    "    create_songplays_table,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_log_data_table = f\"\"\"\n",
    "INSERT INTO log_data (\n",
    "    artist,\n",
    "    auth,\n",
    "    firstName,\n",
    "    gender,\n",
    "    itemInSession,\n",
    "    lastName,\n",
    "    length,\n",
    "    level,\n",
    "    location,\n",
    "    method,\n",
    "    page,\n",
    "    registration,\n",
    "    sessionId,\n",
    "    song,\n",
    "    status,\n",
    "    ts,\n",
    "    userAgent,\n",
    "    userId\n",
    ") VALUES (\n",
    "    ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?,?, ?, ?, ?, ?, ?\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "insert_song_data_table = f\"\"\"\n",
    "INSERT INTO song_data (\n",
    "    artist_id,\n",
    "    artist_latitude,\n",
    "    artist_location,\n",
    "    artist_longitude,\n",
    "    artist_name,\n",
    "    duration,\n",
    "    num_songs,\n",
    "    song_id,\n",
    "    title,\n",
    "    year\n",
    ") VALUES (\n",
    "    ?, ?, ?, ?, ?, ?, ?, ?, ?, ?\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Insert tables\n",
    "insert_tables = [\n",
    "    insert_log_data_table,\n",
    "    insert_song_data_table,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source data paths\n",
    "log_data_path = \"./data/project/log_data.csv\"\n",
    "song_data_path = \"./data/project/song_data.csv\"\n",
    "\n",
    "# Data paths for staging tables\n",
    "data_paths = [\n",
    "    log_data_path,\n",
    "    song_data_path,\n",
    "]\n",
    "\n",
    "# Drop staging tables\n",
    "drop_staging_tables = [\n",
    "    drop_log_data_table,\n",
    "    drop_song_data_table,\n",
    "]\n",
    "\n",
    "# Create staging tables\n",
    "create_staging_tables = [\n",
    "    create_log_data_table,\n",
    "    create_song_data_table,\n",
    "]\n",
    "\n",
    "# Insert staging tables\n",
    "insert_staging_tables = [\n",
    "    insert_log_data_table,\n",
    "    insert_song_data_table,\n",
    "]\n",
    "\n",
    "# Drop all staging tables\n",
    "for query in drop_staging_tables:\n",
    "    cursor.execute(query)\n",
    "\n",
    "# Create all staging tables\n",
    "for query in create_staging_tables:\n",
    "    cursor.execute(query)\n",
    "\n",
    "# Insert all staging tables\n",
    "for i, query in enumerate(insert_staging_tables):\n",
    "    with open(data_paths[i], \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # skip header\n",
    "        for row in reader:\n",
    "            data = [None if x == \"\" else x for x in row]\n",
    "            cursor.execute(query, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get pandas dataframe from sql query\n",
    "def get_df_from_sql(sql_query):\n",
    "    cursor.execute(sql_query)\n",
    "    df = pd.DataFrame(cursor.fetchall())\n",
    "    df.columns = [x[0] for x in cursor.description]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data = get_df_from_sql(\"SELECT * FROM log_data;\")\n",
    "log_data.sort_values(by=[\"artist\", \"song\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all data is inserted correctly\n",
    "(\n",
    "    (get_df_from_sql(\"SELECT * FROM log_data;\").fillna(\"__NA__\") == all_log_data.fillna(\"__NA__\")).all().all() == True,\n",
    "    (get_df_from_sql(\"SELECT * FROM song_data;\").fillna(\"__NA__\") == all_song_data.fillna(\"__NA__\")).all().all() == True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert time query\n",
    "insert_time_table = \"\"\"\n",
    "INSERT INTO \n",
    "    time\n",
    "SELECT \n",
    "    DATETIME(ts / 1000, 'auto')                 AS start_time,\n",
    "    strftime('%Y', DATETIME(ts / 1000, 'auto')) AS year,\n",
    "    strftime('%m', DATETIME(ts / 1000, 'auto')) AS month,\n",
    "    strftime('%d', DATETIME(ts / 1000, 'auto')) AS day,\n",
    "    strftime('%H', DATETIME(ts / 1000, 'auto')) AS hour,\n",
    "    strftime('%W', DATETIME(ts / 1000, 'auto')) AS week,\n",
    "    strftime('%w', DATETIME(ts / 1000, 'auto')) AS weekday\n",
    "FROM \n",
    "    log_data\n",
    "WHERE\n",
    "    auth = 'Logged In' AND \n",
    "    length > 0\n",
    "GROUP BY\n",
    "    start_time\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(drop_time_table)\n",
    "cursor.execute(create_time_table)\n",
    "cursor.execute(insert_time_table)\n",
    "\n",
    "get_df_from_sql(\"SELECT * FROM time;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert users query\n",
    "insert_users_table = \"\"\"\n",
    "INSERT INTO\n",
    "    users\n",
    "SELECT\n",
    "    user_id,\n",
    "    first_name,\n",
    "    last_name,\n",
    "    gender,\n",
    "    level\n",
    "FROM (\n",
    "    SELECT\n",
    "        userId                                  AS user_id,\n",
    "        firstName                               AS first_name,\n",
    "        lastName                                AS last_name,\n",
    "        gender,\n",
    "        level,\n",
    "        DATETIME(ts / 1000, 'auto')             AS time\n",
    "    FROM\n",
    "        log_data\n",
    "    WHERE\n",
    "        auth = 'Logged In' AND \n",
    "        length > 0\n",
    "    )\n",
    "GROUP BY\n",
    "    user_id\n",
    "HAVING \n",
    "    time = MAX(time)\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(drop_users_table)\n",
    "cursor.execute(create_users_table)\n",
    "cursor.execute(insert_users_table)\n",
    "\n",
    "get_df_from_sql(\"SELECT * FROM users;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert artists query\n",
    "insert_artists_table = \"\"\"\n",
    "INSERT INTO\n",
    "    artists\n",
    "SELECT \n",
    "    ROW_NUMBER() OVER ()                        AS artist_id,\n",
    "    name, \n",
    "    location, \n",
    "    latitude, \n",
    "    longitude\n",
    "FROM \n",
    "    (\n",
    "        SELECT DISTINCT \n",
    "            artist                              AS name\n",
    "        FROM\n",
    "            log_data\n",
    "        WHERE\n",
    "            log_data.auth = 'Logged In' AND \n",
    "            log_data.length > 0\n",
    "    )\n",
    "LEFT JOIN \n",
    "    (\n",
    "        SELECT DISTINCT\n",
    "            artist_name,\n",
    "            artist_location                     AS location,\n",
    "            artist_latitude                     AS latitude,\n",
    "            artist_longitude                    AS longitude\n",
    "        FROM\n",
    "            song_data\n",
    "        GROUP BY\n",
    "            artist_name\n",
    "        ORDER BY\n",
    "            location DESC,\n",
    "            latitude DESC,\n",
    "            longitude DESC\n",
    "    ) \n",
    "ON \n",
    "    name = artist_name\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(drop_artists_table)\n",
    "cursor.execute(create_artists_table)\n",
    "cursor.execute(insert_artists_table)\n",
    "\n",
    "get_df_from_sql(\"SELECT * FROM artists;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert songs query\n",
    "insert_songs_table = \"\"\"\n",
    "INSERT INTO\n",
    "    songs\n",
    "SELECT\n",
    "    ROW_NUMBER() OVER ()                        AS song_id,\n",
    "    first_part.title,\n",
    "    first_part.artist_id,\n",
    "    second_part.year,\n",
    "    second_part.duration\n",
    "FROM \n",
    "(\n",
    "    (\n",
    "        (\n",
    "            SELECT\n",
    "                song AS title,\n",
    "                artist\n",
    "            FROM\n",
    "                log_data\n",
    "            WHERE\n",
    "                auth = 'Logged In' AND\n",
    "                length > 0\n",
    "            GROUP BY\n",
    "                title,\n",
    "                artist\n",
    "            ORDER BY\n",
    "                title\n",
    "        )\n",
    "        LEFT JOIN (\n",
    "            SELECT\n",
    "                name,\n",
    "                artist_id\n",
    "            FROM\n",
    "                artists\n",
    "        )\n",
    "        ON\n",
    "            artist = name\n",
    "    ) AS first_part\n",
    "    LEFT JOIN (\n",
    "        SELECT\n",
    "            title,\n",
    "            artist_name,\n",
    "            year,\n",
    "            duration\n",
    "        FROM\n",
    "            song_data\n",
    "        WHERE\n",
    "            year > 0\n",
    "        GROUP BY\n",
    "            title,\n",
    "            artist_name\n",
    "        HAVING\n",
    "            duration = MAX(duration)\n",
    "    ) AS second_part\n",
    "    ON\n",
    "        first_part.title = second_part.title AND\n",
    "        first_part.artist = second_part.artist_name\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(drop_songs_table)\n",
    "cursor.execute(create_songs_table)\n",
    "cursor.execute(insert_songs_table)\n",
    "\n",
    "get_df_from_sql(\"SELECT * FROM songs;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    raw_log_data.session_id,\n",
    "    raw_log_data.item_in_session,\n",
    "    raw_log_data.start_time,\n",
    "    raw_log_data.artist,\n",
    "    raw_artist_data.artist_id,\n",
    "    raw_log_data.song,\n",
    "    raw_song_data.song_id,\n",
    "    raw_log_data.user_id,\n",
    "    raw_log_data.level,\n",
    "    raw_log_data.location,\n",
    "    raw_log_data.user_agent\n",
    "FROM\n",
    "    (   \n",
    "        SELECT\n",
    "            sessionId                           AS session_id,\n",
    "            itemInSession AS item_in_session,\n",
    "            ts AS start_time,\n",
    "            artist,\n",
    "            song,\n",
    "            userId AS user_id,\n",
    "            level,\n",
    "            location,\n",
    "            userAgent                           AS user_agent\n",
    "        FROM\n",
    "            log_data\n",
    "        WHERE\n",
    "            auth = 'Logged In' AND\n",
    "            length > 0\n",
    "    )                                           AS raw_log_data\n",
    "JOIN\n",
    "    (\n",
    "        SELECT\n",
    "            artist_id,\n",
    "            name\n",
    "        FROM\n",
    "            artists\n",
    "    )                                           AS raw_artist_data\n",
    "ON\n",
    "    raw_log_data.artist = raw_artist_data.name\n",
    "JOIN\n",
    "    (\n",
    "        SELECT\n",
    "            song_id,\n",
    "            title,\n",
    "            artist_id\n",
    "        FROM\n",
    "            songs\n",
    "    )                                           AS raw_song_data\n",
    "ON\n",
    "    raw_artist_data.artist_id = raw_song_data.artist_id AND\n",
    "    raw_log_data.song = raw_song_data.title\n",
    "\"\"\"\n",
    "\n",
    "get_df_from_sql(query)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert songplays query\n",
    "insert_songplays_table = \"\"\"\n",
    "INSERT INTO\n",
    "    songplays\n",
    "SELECT\n",
    "    raw_log_data.session_id,\n",
    "    raw_log_data.item_in_session,\n",
    "    raw_log_data.start_time,\n",
    "    raw_artist_data.artist_id,\n",
    "    raw_song_data.song_id,\n",
    "    raw_log_data.user_id,\n",
    "    raw_log_data.level,\n",
    "    raw_log_data.location,\n",
    "    raw_log_data.user_agent\n",
    "FROM\n",
    "    (   \n",
    "        SELECT\n",
    "            sessionId                           AS session_id,\n",
    "            itemInSession                       AS item_in_session,\n",
    "            ts                                  AS start_time,\n",
    "            artist,\n",
    "            song,\n",
    "            userId                              AS user_id,\n",
    "            level,\n",
    "            location,\n",
    "            userAgent                           AS user_agent\n",
    "        FROM\n",
    "            log_data\n",
    "        WHERE\n",
    "            auth = 'Logged In' AND\n",
    "            length > 0\n",
    "    )                                           AS raw_log_data\n",
    "JOIN\n",
    "    (\n",
    "        SELECT\n",
    "            artist_id,\n",
    "            name\n",
    "        FROM\n",
    "            artists\n",
    "    )                                           AS raw_artist_data\n",
    "ON\n",
    "    raw_log_data.artist = raw_artist_data.name\n",
    "JOIN\n",
    "    (\n",
    "        SELECT\n",
    "            song_id,\n",
    "            title,\n",
    "            artist_id\n",
    "        FROM\n",
    "            songs\n",
    "    )                                           AS raw_song_data\n",
    "ON\n",
    "    raw_artist_data.artist_id = raw_song_data.artist_id AND\n",
    "    raw_log_data.song = raw_song_data.title\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(drop_songplays_table)\n",
    "cursor.execute(create_songplays_table)\n",
    "cursor.execute(insert_songplays_table)\n",
    "\n",
    "get_df_from_sql(\"SELECT * FROM songplays;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for query in drop_tables:\n",
    "#    cursor.execute(query)\n",
    "#    connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.commit()\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloud-warehouse--FX0TuTV-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
